{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My data has a unique structure not commonly supported by standard machine learning/deep learning frameworks. Specifically, I have data consisting of 10 features for each of two distinct entities. My goal is to predict the similarity between these two entities based on their features over the past k=5 days. Both entities are evaluated using the same set of 10 features each day.\n",
    "\n",
    "What neural network architecture would be suitable for predicting their similarity?\n",
    "\n",
    "Assume that the available data for the entity 1 is\n",
    "\n",
    "|     | f1 | f2 | f3 | f4 | f5 | f6 | f7 | f8 | f9 | f10 |\n",
    "|-----|----|----|----|----|----|----|----|----|----|-----|\n",
    "| t1  | 5  | 4  | 3  | 2  | 5  | 2  | 4  | 3  | 2  | 1   |\n",
    "| t2  | 7  | 1  | 0  | 2  | 5  | 6  | 4  | 2  | 6  | -1  |\n",
    "| t3  | 1  | 4  | 3  | 1  | 5  | 2  | 4  | 3  | 1  | 2   |\n",
    "| t4  | 7  | 1  | 0  | 2  | 5  | 6  | 4  | 2  | 6  | -1  |\n",
    "| t5  | 5  | 4  | 3  | 0  | 5  | 2  | 1  | 3  | 5  | 1   |\n",
    "| t6  | 2  | 3  | 9  | 4  | 5  | 6  | 4  | 2  | 6  | -17 |\n",
    "| t7  | 6  | 2  | 4  | 0  | 5  | 2  | 4  | 3  | 1  | 2   |\n",
    "| t8  | 5  | 1  | 0  | 2  | 5  | 6  | 4  | 2  | 6  | 0   |\n",
    "| t9  | 5  | 4  | 3  | 0  | 5  | 2  | 1  | 3  | 5  | 1   |\n",
    "| t10 | 2  | 3  | 9  | 0  | 5  | 6  | 4  | 4  | 6  | 7   |\n",
    "| t11 | 6  | 2  | 4  | 3  | 5  | 4  | 4  | 3  | 1  | 8   |\n",
    "| t12 | 5  | 1  | 0  | 2  | 2  | 6  | 4  | 2  | 3  | 0   |\n",
    "\n",
    "Assume that the available data for the entity 2 is\n",
    "\n",
    "|     | f1 | f2 | f3 | f4 | f5 | f6 | f7 | f8 | f9 | f10 |\n",
    "|-----|----|----|----|----|----|----|----|----|----|-----|\n",
    "| t1  | 5  | 4  | 5  | 2  | 5  | 2  | 4  | 3  | 2  | 15  |\n",
    "| t2  | 7  | 1  | 4  | 2  | 5  | 6  | 4  | 2  | 6  | 0   |\n",
    "| t3  | 1  | 4  | 4  | 1  | 5  | 5  | 4  | 3  | 1  | 2   |\n",
    "| t4  | 7  | 1  | 4  | 2  | 5  | 6  | 4  | 2  | 6  | -1  |\n",
    "| t5  | 7  | 4  | 3  | 0  | 5  | 4  | 1  | 3  | 5  | 1   |\n",
    "| t6  | 2  | 7  | 7  | 4  | 5  | 6  | 4  | 2  | 6  | -7  |\n",
    "| t7  | 6  | 7  | 7  | 0  | 5  | 0  | 4  | 3  | 1  | 2   |\n",
    "| t8  | 0  | 1  | 0  | 2  | 7  | 6  | 4  | 0  | 6  | 0   |\n",
    "| t9  | 5  | 4  | 3  | 0  | 5  | 2  | 1  | 3  | 5  | 1   |\n",
    "| t10 | 1  | 3  | 9  | 0  | 5  | 6  | 0  | 4  | 6  | 5   |\n",
    "| t11 | 6  | 2  | 4  | 3  | 5  | 4  | 4  | 3  | 1  | 8   |\n",
    "| t12 | 5  | 1  | 0  | 2  | 2  | 6  | 4  | 2  | 3  | 0   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the similarity between two entities based on their features over the past $ k = 5 $ days, you can use a neural network architecture that effectively captures the temporal dynamics and relationships between the features of both entities. A suitable approach is to use a combination of recurrent neural networks (RNNs) or transformers to process the temporal sequences and a final similarity metric, such as cosine similarity, to measure the similarity between the two processed sequences.\n",
    "\n",
    "Here is a detailed step-by-step outline of a suitable neural network architecture:\n",
    "\n",
    "1. **Input Preparation:**\n",
    "   - For each entity, you will have a sequence of feature vectors over the past 5 days.\n",
    "   - Let $ \\mathbf{X}_1 = [\\mathbf{x}_1(t_{-4}), \\mathbf{x}_1(t_{-3}), \\mathbf{x}_1(t_{-2}), \\mathbf{x}_1(t_{-1}), \\mathbf{x}_1(t_{0})] $ be the sequence of feature vectors for Entity 1.\n",
    "   - Similarly, let $ \\mathbf{X}_2 = [\\mathbf{x}_2(t_{-4}), \\mathbf{x}_2(t_{-3}), \\mathbf{x}_2(t_{-2}), \\mathbf{x}_2(t_{-1}), \\mathbf{x}_2(t_{0})] $ be the sequence of feature vectors for Entity 2.\n",
    "\n",
    "2. **Recurrent Neural Network (RNN) / Transformer Encoder:**\n",
    "   - Use an RNN (e.g., LSTM or GRU) or a Transformer encoder to process the sequence of feature vectors for each entity.\n",
    "   - This will output a fixed-size context vector for each entity that captures the temporal dynamics and feature interactions over the past 5 days.\n",
    "\n",
    "3. **Feature Extraction:**\n",
    "   - The RNN or Transformer encoder will produce two context vectors:\n",
    "     - $ \\mathbf{h}_1 $ for Entity 1\n",
    "     - $ \\mathbf{h}_2 $ for Entity 2\n",
    "\n",
    "4. **Similarity Calculation:**\n",
    "   - Compute the similarity between the two context vectors using a similarity metric such as cosine similarity.\n",
    "   - Cosine similarity can be calculated as:\n",
    "     $$\n",
    "     \\text{similarity}(\\mathbf{h}_1, \\mathbf{h}_2) = \\frac{\\mathbf{h}_1 \\cdot \\mathbf{h}_2}{\\|\\mathbf{h}_1\\| \\|\\mathbf{h}_2\\|}\n",
    "     $$\n",
    "\n",
    "### Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimilarityModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(SimilarityModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        # Encode sequence of entity 1\n",
    "        _, (h_n1, _) = self.lstm(x1)\n",
    "        h1 = self.fc(h_n1[-1])\n",
    "        \n",
    "        # Encode sequence of entity 2\n",
    "        _, (h_n2, _) = self.lstm(x2)\n",
    "        h2 = self.fc(h_n2[-1])\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = F.cosine_similarity(h1, h2)\n",
    "        \n",
    "        return similarity\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 10  # Number of features per day\n",
    "hidden_size = 64 # Size of LSTM hidden state\n",
    "num_layers = 2   # Number of LSTM layers\n",
    "\n",
    "# Initialize the model\n",
    "model = SimilarityModel(input_size, hidden_size, num_layers)\n",
    "\n",
    "# Example input data\n",
    "# x1 and x2 are tensors of shape (batch_size, 5, 10)\n",
    "x1 = torch.tensor([\n",
    "    [\n",
    "        [5, 4, 3, 2, 5, 2, 4, 3, 2, 1],\n",
    "        [7, 1, 0, 2, 5, 6, 4, 2, 6, -1],\n",
    "        [1, 4, 3, 1, 5, 2, 4, 3, 1, 2],\n",
    "        [7, 1, 0, 2, 5, 6, 4, 2, 6, -1],\n",
    "        [5, 4, 3, 0, 5, 2, 1, 3, 5, 1]\n",
    "    ]\n",
    "], dtype=torch.float)\n",
    "\n",
    "x2 = torch.tensor([\n",
    "    [\n",
    "        [5, 4, 5, 2, 5, 2, 4, 3, 2, 15],\n",
    "        [7, 1, 4, 2, 5, 6, 4, 2, 6, 0],\n",
    "        [1, 4, 4, 1, 5, 5, 4, 3, 1, 2],\n",
    "        [7, 1, 4, 2, 5, 6, 4, 2, 6, -1],\n",
    "        [7, 4, 3, 0, 5, 4, 1, 3, 5, 1]\n",
    "    ]\n",
    "], dtype=torch.float)\n",
    "\n",
    "# Predict similarity\n",
    "similarity = model(x1, x2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "1. **Input Preparation:**\n",
    "   - The model expects input tensors \\( x1 \\) and \\( x2 \\) of shape \\( (batch\\_size, 5, 10) \\) representing sequences of feature vectors for both entities over the past 5 days.\n",
    "\n",
    "2. **LSTM Encoding:**\n",
    "   - The LSTM processes each sequence and outputs the hidden states. The final hidden state \\( h_n \\) of the last LSTM layer is used as the context vector for each entity.\n",
    "   \n",
    "3. **Feature Extraction:**\n",
    "   - The context vector \\( h_n \\) is passed through a fully connected layer to produce the final feature vectors \\( h1 \\) and \\( h2 \\) for each entity.\n",
    "\n",
    "4. **Cosine Similarity Calculation:**\n",
    "   - The cosine similarity between the final feature vectors \\( h1 \\) and \\( h2 \\) is computed to predict the similarity between the two entities.\n",
    "\n",
    "This architecture effectively captures the temporal dynamics of the features for each entity and computes a similarity score based on their processed sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Transformer Encoder\n",
    "\n",
    "The TransformerEncoder processes each sequence and outputs the encoded representations for each day. The mean of these encoded representations is taken to obtain a fixed-size context vector for each entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimilarityTransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_heads, dropout=0.1):\n",
    "        super(SimilarityTransformerModel, self).__init__()\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=input_size, nhead=num_heads, dim_feedforward=hidden_size, dropout=dropout),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        # Encode sequence of entity 1\n",
    "        x1 = self.transformer_encoder(x1)\n",
    "        h1 = self.fc(x1.mean(dim=1))  # Taking the mean of the sequence\n",
    "        \n",
    "        # Encode sequence of entity 2\n",
    "        x2 = self.transformer_encoder(x2)\n",
    "        h2 = self.fc(x2.mean(dim=1))  # Taking the mean of the sequence\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = F.cosine_similarity(h1, h2)\n",
    "        \n",
    "        return similarity\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 10  # Number of features per day\n",
    "hidden_size = 64 # Size of hidden layer\n",
    "num_layers = 2   # Number of transformer layers\n",
    "num_heads = 2    # Number of attention heads\n",
    "\n",
    "# Initialize the model\n",
    "model = SimilarityTransformerModel(input_size, hidden_size, num_layers, num_heads)\n",
    "\n",
    "# Example input data\n",
    "# x1 and x2 are tensors of shape (batch_size, 5, 10)\n",
    "x1 = torch.tensor([\n",
    "    [\n",
    "        [5, 4, 3, 2, 5, 2, 4, 3, 2, 1],\n",
    "        [7, 1, 0, 2, 5, 6, 4, 2, 6, -1],\n",
    "        [1, 4, 3, 1, 5, 2, 4, 3, 1, 2],\n",
    "        [7, 1, 0, 2, 5, 6, 4, 2, 6, -1],\n",
    "        [5, 4, 3, 0, 5, 2, 1, 3, 5, 1]\n",
    "    ]\n",
    "], dtype=torch.float)\n",
    "\n",
    "x2 = torch.tensor([\n",
    "    [\n",
    "        [5, 4, 5, 2, 5, 2, 4, 3, 2, 15],\n",
    "        [7, 1, 4, 2, 5, 6, 4, 2, 6, 0],\n",
    "        [1, 4, 4, 1, 5, 5, 4, 3, 1, 2],\n",
    "        [7, 1, 4, 2, 5, 6, 4, 2, 6, -1],\n",
    "        [7, 4, 3, 0, 5, 4, 1, 3, 5, 1]\n",
    "    ]\n",
    "], dtype=torch.float)\n",
    "\n",
    "# Predict similarity\n",
    "similarity = model(x1, x2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
