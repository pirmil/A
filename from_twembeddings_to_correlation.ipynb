{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My data has a unique structure not commonly supported by standard machine learning/deep learning frameworks. Specifically, I have data consisting of 10 features for each of two distinct entities. My goal is to predict the similarity between these two entities based on their features over the past k=5 days. Both entities are evaluated using the same set of 10 features each day.\n",
    "\n",
    "What neural network architecture would be suitable for predicting their similarity?\n",
    "\n",
    "Assume that the available data for the entity 1 is\n",
    "\n",
    "|     | f1 | f2 | f3 | f4 | f5 | f6 | f7 | f8 | f9 | f10 |\n",
    "|-----|----|----|----|----|----|----|----|----|----|-----|\n",
    "| t1  | 5  | 4  | 3  | 2  | 5  | 2  | 4  | 3  | 2  | 1   |\n",
    "| t2  | 7  | 1  | 0  | 2  | 5  | 6  | 4  | 2  | 6  | -1  |\n",
    "| t3  | 1  | 4  | 3  | 1  | 5  | 2  | 4  | 3  | 1  | 2   |\n",
    "| t4  | 7  | 1  | 0  | 2  | 5  | 6  | 4  | 2  | 6  | -1  |\n",
    "| t5  | 5  | 4  | 3  | 0  | 5  | 2  | 1  | 3  | 5  | 1   |\n",
    "| t6  | 2  | 3  | 9  | 4  | 5  | 6  | 4  | 2  | 6  | -17 |\n",
    "| t7  | 6  | 2  | 4  | 0  | 5  | 2  | 4  | 3  | 1  | 2   |\n",
    "| t8  | 5  | 1  | 0  | 2  | 5  | 6  | 4  | 2  | 6  | 0   |\n",
    "| t9  | 5  | 4  | 3  | 0  | 5  | 2  | 1  | 3  | 5  | 1   |\n",
    "| t10 | 2  | 3  | 9  | 0  | 5  | 6  | 4  | 4  | 6  | 7   |\n",
    "| t11 | 6  | 2  | 4  | 3  | 5  | 4  | 4  | 3  | 1  | 8   |\n",
    "| t12 | 5  | 1  | 0  | 2  | 2  | 6  | 4  | 2  | 3  | 0   |\n",
    "\n",
    "Assume that the available data for the entity 2 is\n",
    "\n",
    "|     | f1 | f2 | f3 | f4 | f5 | f6 | f7 | f8 | f9 | f10 |\n",
    "|-----|----|----|----|----|----|----|----|----|----|-----|\n",
    "| t1  | 5  | 4  | 5  | 2  | 5  | 2  | 4  | 3  | 2  | 15  |\n",
    "| t2  | 7  | 1  | 4  | 2  | 5  | 6  | 4  | 2  | 6  | 0   |\n",
    "| t3  | 1  | 4  | 4  | 1  | 5  | 5  | 4  | 3  | 1  | 2   |\n",
    "| t4  | 7  | 1  | 4  | 2  | 5  | 6  | 4  | 2  | 6  | -1  |\n",
    "| t5  | 7  | 4  | 3  | 0  | 5  | 4  | 1  | 3  | 5  | 1   |\n",
    "| t6  | 2  | 7  | 7  | 4  | 5  | 6  | 4  | 2  | 6  | -7  |\n",
    "| t7  | 6  | 7  | 7  | 0  | 5  | 0  | 4  | 3  | 1  | 2   |\n",
    "| t8  | 0  | 1  | 0  | 2  | 7  | 6  | 4  | 0  | 6  | 0   |\n",
    "| t9  | 5  | 4  | 3  | 0  | 5  | 2  | 1  | 3  | 5  | 1   |\n",
    "| t10 | 1  | 3  | 9  | 0  | 5  | 6  | 0  | 4  | 6  | 5   |\n",
    "| t11 | 6  | 2  | 4  | 3  | 5  | 4  | 4  | 3  | 1  | 8   |\n",
    "| t12 | 5  | 1  | 0  | 2  | 2  | 6  | 4  | 2  | 3  | 0   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the similarity between two entities based on their features over the past $ k = 5 $ days, you can use a neural network architecture that effectively captures the temporal dynamics and relationships between the features of both entities. A suitable approach is to use a combination of recurrent neural networks (RNNs) or transformers to process the temporal sequences and a final similarity metric, such as cosine similarity, to measure the similarity between the two processed sequences.\n",
    "\n",
    "Here is a detailed step-by-step outline of a suitable neural network architecture:\n",
    "\n",
    "1. **Input Preparation:**\n",
    "   - For each entity, you will have a sequence of feature vectors over the past 5 days.\n",
    "   - Let $ \\mathbf{X}_1 = [\\mathbf{x}_1(t_{-4}), \\mathbf{x}_1(t_{-3}), \\mathbf{x}_1(t_{-2}), \\mathbf{x}_1(t_{-1}), \\mathbf{x}_1(t_{0})] $ be the sequence of feature vectors for Entity 1.\n",
    "   - Similarly, let $ \\mathbf{X}_2 = [\\mathbf{x}_2(t_{-4}), \\mathbf{x}_2(t_{-3}), \\mathbf{x}_2(t_{-2}), \\mathbf{x}_2(t_{-1}), \\mathbf{x}_2(t_{0})] $ be the sequence of feature vectors for Entity 2.\n",
    "\n",
    "2. **Recurrent Neural Network (RNN) / Transformer Encoder:**\n",
    "   - Use an RNN (e.g., LSTM or GRU) or a Transformer encoder to process the sequence of feature vectors for each entity.\n",
    "   - This will output a fixed-size context vector for each entity that captures the temporal dynamics and feature interactions over the past 5 days.\n",
    "\n",
    "3. **Feature Extraction:**\n",
    "   - The RNN or Transformer encoder will produce two context vectors:\n",
    "     - $ \\mathbf{h}_1 $ for Entity 1\n",
    "     - $ \\mathbf{h}_2 $ for Entity 2\n",
    "\n",
    "4. **Similarity Calculation:**\n",
    "   - Compute the similarity between the two context vectors using a similarity metric such as cosine similarity.\n",
    "   - Cosine similarity can be calculated as:\n",
    "     $$\n",
    "     \\text{similarity}(\\mathbf{h}_1, \\mathbf{h}_2) = \\frac{\\mathbf{h}_1 \\cdot \\mathbf{h}_2}{\\|\\mathbf{h}_1\\| \\|\\mathbf{h}_2\\|}\n",
    "     $$\n",
    "\n",
    "### Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimilarityModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(SimilarityModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        # Encode sequence of entity 1\n",
    "        _, (h_n1, _) = self.lstm(x1)\n",
    "        h1 = self.fc(h_n1[-1])\n",
    "        \n",
    "        # Encode sequence of entity 2\n",
    "        _, (h_n2, _) = self.lstm(x2)\n",
    "        h2 = self.fc(h_n2[-1])\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = F.cosine_similarity(h1, h2)\n",
    "        \n",
    "        return similarity\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 10  # Number of features per day\n",
    "hidden_size = 64 # Size of LSTM hidden state\n",
    "num_layers = 2   # Number of LSTM layers\n",
    "\n",
    "# Initialize the model\n",
    "model = SimilarityModel(input_size, hidden_size, num_layers)\n",
    "\n",
    "# Example input data\n",
    "# x1 and x2 are tensors of shape (batch_size, 5, 10)\n",
    "x1 = torch.tensor([\n",
    "    [\n",
    "        [5, 4, 3, 2, 5, 2, 4, 3, 2, 1],\n",
    "        [7, 1, 0, 2, 5, 6, 4, 2, 6, -1],\n",
    "        [1, 4, 3, 1, 5, 2, 4, 3, 1, 2],\n",
    "        [7, 1, 0, 2, 5, 6, 4, 2, 6, -1],\n",
    "        [5, 4, 3, 0, 5, 2, 1, 3, 5, 1]\n",
    "    ]\n",
    "], dtype=torch.float)\n",
    "\n",
    "x2 = torch.tensor([\n",
    "    [\n",
    "        [5, 4, 5, 2, 5, 2, 4, 3, 2, 15],\n",
    "        [7, 1, 4, 2, 5, 6, 4, 2, 6, 0],\n",
    "        [1, 4, 4, 1, 5, 5, 4, 3, 1, 2],\n",
    "        [7, 1, 4, 2, 5, 6, 4, 2, 6, -1],\n",
    "        [7, 4, 3, 0, 5, 4, 1, 3, 5, 1]\n",
    "    ]\n",
    "], dtype=torch.float)\n",
    "\n",
    "# Predict similarity\n",
    "similarity = model(x1, x2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "1. **Input Preparation:**\n",
    "   - The model expects input tensors \\( x1 \\) and \\( x2 \\) of shape \\( (batch\\_size, 5, 10) \\) representing sequences of feature vectors for both entities over the past 5 days.\n",
    "\n",
    "2. **LSTM Encoding:**\n",
    "   - The LSTM processes each sequence and outputs the hidden states. The final hidden state \\( h_n \\) of the last LSTM layer is used as the context vector for each entity.\n",
    "   \n",
    "3. **Feature Extraction:**\n",
    "   - The context vector \\( h_n \\) is passed through a fully connected layer to produce the final feature vectors \\( h1 \\) and \\( h2 \\) for each entity.\n",
    "\n",
    "4. **Cosine Similarity Calculation:**\n",
    "   - The cosine similarity between the final feature vectors \\( h1 \\) and \\( h2 \\) is computed to predict the similarity between the two entities.\n",
    "\n",
    "This architecture effectively captures the temporal dynamics of the features for each entity and computes a similarity score based on their processed sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Transformer Encoder\n",
    "\n",
    "The TransformerEncoder processes each sequence and outputs the encoded representations for each day. The mean of these encoded representations is taken to obtain a fixed-size context vector for each entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimilarityTransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_heads, dropout=0.1):\n",
    "        super(SimilarityTransformerModel, self).__init__()\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=input_size, nhead=num_heads, dim_feedforward=hidden_size, dropout=dropout),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        # Encode sequence of entity 1\n",
    "        x1 = self.transformer_encoder(x1)\n",
    "        h1 = self.fc(x1.mean(dim=1))  # Taking the mean of the sequence\n",
    "        \n",
    "        # Encode sequence of entity 2\n",
    "        x2 = self.transformer_encoder(x2)\n",
    "        h2 = self.fc(x2.mean(dim=1))  # Taking the mean of the sequence\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = F.cosine_similarity(h1, h2)\n",
    "        \n",
    "        return similarity\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 10  # Number of features per day\n",
    "hidden_size = 64 # Size of hidden layer\n",
    "num_layers = 2   # Number of transformer layers\n",
    "num_heads = 2    # Number of attention heads\n",
    "\n",
    "# Initialize the model\n",
    "model = SimilarityTransformerModel(input_size, hidden_size, num_layers, num_heads)\n",
    "\n",
    "# Example input data\n",
    "# x1 and x2 are tensors of shape (batch_size, 5, 10)\n",
    "x1 = torch.tensor([\n",
    "    [\n",
    "        [5, 4, 3, 2, 5, 2, 4, 3, 2, 1],\n",
    "        [7, 1, 0, 2, 5, 6, 4, 2, 6, -1],\n",
    "        [1, 4, 3, 1, 5, 2, 4, 3, 1, 2],\n",
    "        [7, 1, 0, 2, 5, 6, 4, 2, 6, -1],\n",
    "        [5, 4, 3, 0, 5, 2, 1, 3, 5, 1]\n",
    "    ]\n",
    "], dtype=torch.float)\n",
    "\n",
    "x2 = torch.tensor([\n",
    "    [\n",
    "        [5, 4, 5, 2, 5, 2, 4, 3, 2, 15],\n",
    "        [7, 1, 4, 2, 5, 6, 4, 2, 6, 0],\n",
    "        [1, 4, 4, 1, 5, 5, 4, 3, 1, 2],\n",
    "        [7, 1, 4, 2, 5, 6, 4, 2, 6, -1],\n",
    "        [7, 4, 3, 0, 5, 4, 1, 3, 5, 1]\n",
    "    ]\n",
    "], dtype=torch.float)\n",
    "\n",
    "# Predict similarity\n",
    "similarity = model(x1, x2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple example of siamese network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps:\n",
    "\n",
    "1. **Dataset Preparation**:\n",
    "    - We'll use pairs of images from the MNIST dataset, labeled as either similar (same digit) or dissimilar (different digits).\n",
    "  \n",
    "2. **Model Definition**:\n",
    "    - Define a simple convolutional neural network (CNN) as the subnetwork.\n",
    "    - Use a distance metric to compare the outputs of the two subnetworks.\n",
    "  \n",
    "3. **Loss Function**:\n",
    "    - Use the contrastive loss to train the network.\n",
    "\n",
    "### Implementation in PyTorch\n",
    "\n",
    "Hereâ€™s a step-by-step implementation:\n",
    "\n",
    "#### 1. Import Required Libraries\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import random\n",
    "```\n",
    "\n",
    "#### 2. Define the Dataset Class\n",
    "\n",
    "```python\n",
    "class SiameseMNIST(Dataset):\n",
    "    def __init__(self, mnist_dataset):\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "        self.data = self.mnist_dataset.data\n",
    "        self.targets = self.mnist_dataset.targets\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img1, label1 = self.data[index], self.targets[index]\n",
    "        should_get_same_class = random.randint(0, 1)\n",
    "        if should_get_same_class:\n",
    "            while True:\n",
    "                index2 = random.randint(0, len(self.data) - 1)\n",
    "                if self.targets[index2] == label1:\n",
    "                    break\n",
    "        else:\n",
    "            while True:\n",
    "                index2 = random.randint(0, len(self.data) - 1)\n",
    "                if self.targets[index2] != label1:\n",
    "                    break\n",
    "        img2, label2 = self.data[index2], self.targets[index2]\n",
    "        img1 = self.transform(img1)\n",
    "        img2 = self.transform(img2)\n",
    "        return img1, img2, torch.tensor([int(label1 == label2)], dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset)\n",
    "```\n",
    "\n",
    "#### 3. Define the Siamese Network\n",
    "\n",
    "```python\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32 * 7 * 7, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_one(input1)\n",
    "        output2 = self.forward_one(input2)\n",
    "        return output1, output2\n",
    "```\n",
    "\n",
    "#### 4. Contrastive Loss Function\n",
    "\n",
    "```python\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = nn.functional.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean(\n",
    "            (1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "            (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "        )\n",
    "        return loss_contrastive\n",
    "```\n",
    "\n",
    "#### 5. Training the Siamese Network\n",
    "\n",
    "```python\n",
    "# Load MNIST dataset\n",
    "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "train_dataset = SiameseMNIST(mnist_train)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=64)\n",
    "\n",
    "# Model, loss, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SiameseNetwork().to(device)\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for img1, img2, label in train_loader:\n",
    "        img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output1, output2 = model(img1, img2)\n",
    "        loss = criterion(output1, output2, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "1. **Dataset Class**:\n",
    "    - `SiameseMNIST` is a custom dataset class that creates pairs of images from the MNIST dataset. Each pair is labeled as similar or dissimilar based on whether the digits are the same.\n",
    "  \n",
    "2. **Model Definition**:\n",
    "    - `SiameseNetwork` consists of a CNN followed by fully connected layers. The `forward` method processes two inputs through the same subnetwork and produces two output feature vectors.\n",
    "  \n",
    "3. **Loss Function**:\n",
    "    - `ContrastiveLoss` calculates the contrastive loss based on the Euclidean distance between the output feature vectors of the two images.\n",
    "\n",
    "4. **Training Loop**:\n",
    "    - The training loop loads the data, processes it through the network, computes the loss, and updates the network weights using backpropagation.\n",
    "\n",
    "This simple example demonstrates how to set up and train a Siamese network for the task of image similarity using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
